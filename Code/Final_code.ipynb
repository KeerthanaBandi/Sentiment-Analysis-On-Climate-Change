{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7671e25",
   "metadata": {},
   "source": [
    "### Finding the subreddits which are related to climate change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf69684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GlobalWarming', 'climate', 'climatechange', 'environment', 'collapse', 'climate_science', 'changemyview', 'climateskeptics', 'ExtinctionRebellion', 'politics']\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Set up the Reddit API\n",
    "user_agent = 'MyApp/1.0 by climate change'\n",
    "reddit = praw.Reddit(\n",
    "    client_id = '4xIYX_X2iMiwGhP4aZtE2g',\n",
    "    client_secret = 'xLymU0PVKlmS3cofzsEKG1fvsiWyrw',\n",
    "    user_agent = user_agent\n",
    ")\n",
    "\n",
    "keywords = ['Global warming', 'Climate change']\n",
    "\n",
    "# Search for subreddits related to the keywords\n",
    "subreddits = []\n",
    "for sr in reddit.subreddits.search(' OR '.join(keywords),limit = 10):\n",
    "    subreddits.append(sr.display_name)\n",
    "\n",
    "print(subreddits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89ce53",
   "metadata": {},
   "source": [
    "### Used PRAW to scrape the data from the subreddits and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e427b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Set up the Reddit API\n",
    "user_agent = 'MyApp/1.0 by climate change'\n",
    "reddit = praw.Reddit(\n",
    "    client_id = '4xIYX_X2iMiwGhP4aZtE2g',\n",
    "    client_secret = 'xLymU0PVKlmS3cofzsEKG1fvsiWyrw',\n",
    "    user_agent = user_agent\n",
    ")\n",
    "\n",
    "# Define the subreddits and search keywords\n",
    "subreddits = [\"climatechange\",\"globalwarming\",\"climate\", \"environment\",\"climate_science\",\"climateskeptics\",\"ClimateOffensive\"]\n",
    "keywords = [\"climate change OR global warming\"]\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = \"2018-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "# Convert the dates to Unix timestamps\n",
    "start_time = int(time.mktime(time.strptime(start_date, '%Y-%m-%d')))\n",
    "end_time = int(time.mktime(time.strptime(end_date, '%Y-%m-%d'))) \n",
    "\n",
    "# Create a list to store the posts\n",
    "posts_list = []\n",
    "\n",
    "# Loop through each subreddit\n",
    "for subreddit in subreddits:\n",
    "    # Search for the posts that match the keywords within the date range\n",
    "    for post in reddit.subreddit(subreddit).search(query=\" OR \".join(keywords), time_filter='all', limit=None):\n",
    "        if start_time <= post.created_utc <= end_time:\n",
    "            # Add the post to the list\n",
    "            posts_list.append([post.title, post.author.name if post.author else 'N/A', post.created_utc, post.ups, subreddit])\n",
    "\n",
    "# Write the posts to a CSV file\n",
    "with open('posts.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "\n",
    "    # Write headers for each column\n",
    "    writer.writerow(['title', 'username', 'date', 'upvotes', 'subreddit'])\n",
    "\n",
    "    # Write data for each post\n",
    "    for post in posts_list:\n",
    "        writer.writerow([post[0], post[1], time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(post[2])), post[3], post[4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41513a55",
   "metadata": {},
   "source": [
    "### Preprocessing - Removed lowercase, URL's and punctuations for better analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa15d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Open the posts.csv file and create a CSV reader object\n",
    "with open('posts.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "    # Skip the header row\n",
    "    next(reader)\n",
    "\n",
    "    # Create a set to store the unique post titles\n",
    "    titles_set = set()\n",
    "\n",
    "    # Create a list to store the preprocessed posts\n",
    "    preprocessed_posts_list = []\n",
    "\n",
    "    # Loop through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # Get the post title from the row\n",
    "        title = row[0]\n",
    "\n",
    "        # Preprocess the post title\n",
    "        title = title.lower() # convert to lowercase\n",
    "        title = re.sub(r'http\\S+', '', title) # remove URLs\n",
    "        #title = re.sub(r'[^\\w\\s]', '', title) # remove punctuations\n",
    "\n",
    "        # Check if the preprocessed title contains any of the keywords\n",
    "        if \"climate change\" in title or \"global warming\" in title:\n",
    "            # Check if the title is already in the set\n",
    "            if title not in titles_set:\n",
    "                # Add the title to the set\n",
    "                titles_set.add(title)\n",
    "                # Add the preprocessed post to the list\n",
    "                preprocessed_posts_list.append([title, row[1], row[2], row[3], row[4]])\n",
    "\n",
    "# Write the preprocessed posts to a new CSV file\n",
    "with open('preprocessed_posts.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "\n",
    "    # Write headers for each column\n",
    "    writer.writerow(['title', 'username', 'date', 'upvotes', 'subreddit'])\n",
    "\n",
    "    # Write data for each preprocessed post\n",
    "    for post in preprocessed_posts_list:\n",
    "        writer.writerow([post[0], post[1], post[2], post[3], post[4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55353a02",
   "metadata": {},
   "source": [
    "### Displaying the first few rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c5b146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title           username  \\\n",
      "0  this is how many trees we would need to fix cl...             R2_XD_   \n",
      "1  some interesting reads i found surrounding cli...        TooneyChaos   \n",
      "2  i’m 100% that climate change/global warming is...  shroomalatians316   \n",
      "3  here’s a mega-list of reputable sources provin...      TheJoshWatson   \n",
      "4  can anyone recommend a list of companies who a...           kuttippa   \n",
      "\n",
      "                  date  upvotes      subreddit  \n",
      "0  2021-11-30 19:14:25        6  climatechange  \n",
      "1  2021-12-13 19:56:42        6  climatechange  \n",
      "2  2019-01-29 15:54:12       43  climatechange  \n",
      "3  2019-10-27 18:59:04      162  climatechange  \n",
      "4  2021-02-21 10:05:25        4  climatechange  \n",
      "           upvotes\n",
      "count   574.000000\n",
      "mean    118.052265\n",
      "std     524.325333\n",
      "min       0.000000\n",
      "25%       6.000000\n",
      "50%      21.000000\n",
      "75%      60.750000\n",
      "max    9778.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the CSV file\n",
    "csv_file = 'preprocessed_posts.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Displaying the first five rows of the data\n",
    "print(df.head())\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf4fc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/keerthanabandi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a163dfb",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d5f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "\n",
    "# Load the preprocessed posts from the CSV file\n",
    "df = pd.read_csv('preprocessed_posts.csv')\n",
    "\n",
    "# Initialize the Vader sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply Vader sentiment analysis to each post\n",
    "sentiment_scores = []\n",
    "for post in df['title']:\n",
    "    # Calculate the sentiment scores\n",
    "    scores = analyzer.polarity_scores(post)\n",
    "    sentiment_scores.append(scores)\n",
    "\n",
    "# Add the sentiment scores to the DataFrame\n",
    "df = pd.concat([df, pd.DataFrame(sentiment_scores)], axis=1)\n",
    "\n",
    "# Categorize the compound score\n",
    "df['sentiment'] = df['compound'].apply(lambda score: 'positive' if score > 0 else ('negative' if score < 0 else 'neutral'))\n",
    "\n",
    "# Rename the columns\n",
    "df = df.rename(columns={'neg': 'negative', 'neu': 'neutral', 'pos': 'positive', 'compound': 'compound_score'})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('vader.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9782e",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c56a604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the input file into a pandas dataframe\n",
    "df = pd.read_csv('preprocessed_posts.csv')\n",
    "\n",
    "# Defining a function to get the sentiment score of each post using TextBlob\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_score = blob.sentiment.polarity\n",
    "    return sentiment_score\n",
    "\n",
    "# Applying the function to each post in the dataframe\n",
    "df['Sentiment Score'] = df['title'].apply(get_sentiment)\n",
    "\n",
    "# Creating a new column to categorize the sentiment scores as positive, negative or neutral\n",
    "df['Sentiment'] = df['Sentiment Score'].apply(lambda x: 'Positive' if x > 0 else 'Negative' if x < 0 else 'Neutral')\n",
    "\n",
    "# Writing the updated dataframe to the output file\n",
    "df.to_csv('textblob.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19e2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571c049e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
